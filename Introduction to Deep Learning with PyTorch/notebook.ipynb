{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Introduction to PyTorch, a Deep Learning Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning vs. deep learning\n",
    "\n",
    "Machine learning and deep learning can both be described as artificial intelligence. In both cases, a computer system learns from data to make intelligent decisions. However, deep learning diverges from machine learning in many different aspects.\n",
    "\n",
    "**Traditional machine learning**\n",
    "- Typically relies on hand-crafted feature engineering\n",
    "- Requires relatively less data\n",
    "- Does not require a GPU\n",
    "\n",
    "**Deep learning**\n",
    "- Can learn features from large, unstructured data\n",
    "- Can extract patterns from audio signals\n",
    "- Models have a lot of parameters\n",
    "- Often requires a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors and accessing attributes\n",
    "\n",
    "Tensors are the primary data structure in PyTorch and will be the building blocks for our deep learning models. They share many similarities with NumPy arrays but have some unique attributes too.\n",
    "\n",
    "In this exercise, you'll practice creating a tensor from a Python list and displaying some of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "cpu\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "list_a = [1, 2, 3, 4]\n",
    "\n",
    "# Create a tensor from list_a\n",
    "tensor_a = torch.tensor(list_a)\n",
    "\n",
    "print(tensor_a)\n",
    "\n",
    "# Display the tensor device\n",
    "print(tensor_a.device)\n",
    "\n",
    "# Display the tensor data type\n",
    "print(tensor_a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors from NumPy arrays\n",
    "\n",
    "Tensors are the fundamental data structure of PyTorch. You can create complex deep learning algorithms by learning how to manipulate them.\n",
    "\n",
    "The `torch` package has been imported, and two NumPy arrays have been created, named `array_a` and `array_b`. Both arrays have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_a = np.array([\n",
    "    [1, 1, 1],\n",
    "    [2, 3, 4],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "array_b = np.array([\n",
    "    [7, 5, 4],\n",
    "    [2, 2, 8],\n",
    "    [6, 3, 8]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1],\n",
      "        [ 4,  7, 28],\n",
      "        [22, 17, 46]])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors from the arrays\n",
    "tensor_a = torch.tensor(array_a)\n",
    "tensor_b = torch.tensor(array_b)\n",
    "\n",
    "# Subtract tensor_b from tensor_a \n",
    "tensor_c = tensor_a - tensor_b\n",
    "\n",
    "# Multiply each element of tensor_a with each element of tensor_b\n",
    "tensor_d = tensor_a * tensor_b\n",
    "\n",
    "# Add tensor_c with tensor_d\n",
    "tensor_e = tensor_c + tensor_d\n",
    "print(tensor_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first neural network\n",
    "\n",
    "In this exercise, you will implement a small neural network containing two *linear* layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor.\n",
    "\n",
    "The `torch` package and the `torch.nn` package have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2025]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 14),\n",
    "    nn.Linear(14, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking linear layers\n",
    "\n",
    "Nice work building your first network with two linear layers. Let's stack some more layers. Remember that a neural network can have as many hidden layers as we want, provided the inputs and outputs line up.\n",
    "\n",
    "The aim of this exercise is for you to become comfortable thinking about the inputs and outputs of each successive layer in a PyTorch neural network.\n",
    "\n",
    "This network is designed to ingest the following input:\n",
    "```py\n",
    "input_tensor = torch.Tensor(\n",
    "    [[2, 3, 6, 7, 9, 3, 2, 1, 5, 3, 6, 9]]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(12, 20),\n",
    "    nn.Linear(20, 14),\n",
    "    nn.Linear(14, 3),\n",
    "    nn.Linear(3, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate your understanding!\n",
    "\n",
    "Neural networks are a core component of deep learning models. They can power so much in your daily life, from language translation apps to the cameras on your smartphone. Which of the following statements about neural networks is true?\n",
    "\n",
    "- [X] A neural network with a single linear layer followed by a sigmoid activation is similar to a logistic regression model.\n",
    "- [ ] A neural network can contain only two linear layers.\n",
    "- [ ] The softmax function takes a tensor of dimension N as input and outputs a float between zero and one.\n",
    "- [X] The input dimension of a linear layer must be equal to the output dimension of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid and softmax functions\n",
    "\n",
    "The sigmoid and softmax functions are two of the most popular activation functions in deep learning. They are both usually used as the last step of a neural network. Sigmoid functions are used for binary classification problems, whereas softmax functions are often used for multi-class classification problems. This exercise will familiarize you with creating and using both functions.\n",
    "\n",
    "Let's say that you have a neural network that returned the values contained in the `score` tensor as a pre-activation output. You will apply activation functions to this output.\n",
    "\n",
    "torch.nn is already imported as `nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6900]])\n",
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Create a sigmoid function and apply it on input_tensor\n",
    "input_tensor = torch.tensor([[0.8]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Training Our First Neural Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a binary classifier in PyTorch\n",
    "\n",
    "Recall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.\n",
    "\n",
    "In this exercise, you'll practice building this small network and interpreting the output of the classifier.\n",
    "\n",
    "The `torch` package and the `torch.nn` package have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9359]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From regression to multi-class classification\n",
    "\n",
    "Recall that the models we have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.\n",
    "\n",
    "In this exercise, you'll start by building a model for regression, and then tweak the model to perform a multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2693]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 7),\n",
    "    nn.Linear(7, 14),\n",
    "    nn.Linear(14, 21),\n",
    "    nn.Linear(21, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8844, 0.0035, 0.1028, 0.0093]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating one-hot encoded labels\n",
    "\n",
    "One-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset. This vector only contains zeros and ones. In this exercise, you'll create the one-hot encoded vector of the label `y` provided.\n",
    "\n",
    "You'll practice doing this manually, and then make your life easier by leveraging the help of PyTorch! Your dataset contains three classes.\n",
    "\n",
    "NumPy is already imported as `np`, and torch.nn.functional as `F`. The `torch` package is also imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(1), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating cross entropy loss\n",
    "\n",
    "Cross entropy loss is the most used loss for classification problems. In this exercise, you will create inputs and calculate cross entropy loss in PyTorch. You are provided with the ground truth label `y` and a vector of `scores` predicted by your model.\n",
    "\n",
    "You'll start by creating a one-hot encoded vector of the ground truth label `y`, which is a required step to compare `y` with the scores predicted by your model. Next, you'll create a cross entropy loss function. Last, you'll call the loss function, which takes `scores` (model predictions before the final softmax function), and the one-hot encoded ground truth label, as inputs. It outputs a single float, the loss of that sample.\n",
    "\n",
    "`torch`, `torch.nn` as `nn`, and `torch.nn.functional` as `F` have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating a sample\n",
    "\n",
    "In previous exercises, you used linear layers to build networks.\n",
    "\n",
    "Recall that the operation performed by `nn.Linear()` is to take an input *X* and apply the transformation *W \\* X + b*, where *W* and *b* are two tensors (called the weight and bias).\n",
    "\n",
    "A critical part of training PyTorch models is to calculate gradients of the weight and bias tensors with respect to a loss function.\n",
    "\n",
    "In this exercise, you will calculate weight and bias tensor gradients using cross entropy loss and a sample of data.\n",
    "\n",
    "The following tensors are provided:\n",
    "- `weight`: a 2x9-element tensor\n",
    "- `bias`: a 2-element tensor\n",
    "- `preds`: a 1x2-element tensor containing the model predictions\n",
    "- `target`: a 1x2-element one-hot encoded tensor containing the ground-truth label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the model parameters\n",
    "\n",
    "A PyTorch model created with the `nn.Sequential()` is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different *linear* layers of a neural network. You won't be accessing the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the weights manually\n",
    "\n",
    "Now that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. PyTorch functions can do what you're about to do, but it's helpful to do the work manually at least once, to understand what's going on under the hood.\n",
    "\n",
    "A neural network of three layers has been created and stored as the `model` variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, `lr`, has been chosen to scale the gradients when performing the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Linear(in_features=16, out_features=8, bias=True),\n",
    "  nn.Linear(in_features=8, out_features=4, bias=True),\n",
    "  nn.Linear(in_features=4, out_features=2, bias=True)\n",
    ")\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr * grads0\n",
    "weight1 = weight1 - lr * grads1\n",
    "weight2 = weight2 - lr * grads2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the PyTorch optimizer\n",
    "\n",
    "In the previous exercise, you manually updated the weight of a network. You now know what's going on under the hood, but this approach is not scalable to a network of many layers.\n",
    "\n",
    "Thankfully, the PyTorch SGD optimizer does a similar job in a handful of lines of code. In this exercise, you will practice the last step to complete the training loop: updating the weights using a PyTorch optimizer.\n",
    "\n",
    "A neural network has been created and provided as the `model` variable. This model was used to run a forward pass and create the tensor of predictions `pred`. The one-hot encoded tensor is named `target` and the cross entropy loss function is stored as `criterion`.\n",
    "\n",
    "`torch.optim` as `optim`, and `torch.nn` as `nn` have already been loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the MSELoss\n",
    "\n",
    "Recall that we can't use cross-entropy loss for regression problems. The mean squared error loss (MSELoss) is a common loss function for regression problems. In this exercise, you will practice calculating and observing the loss using NumPy as well as its PyTorch implementation.\n",
    "\n",
    "The `torch` package has been imported as well as `numpy` as `np` and `torch.nn` as `nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y - y_hat)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).double(), torch.tensor(y).double())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a training loop\n",
    "\n",
    "In scikit-learn, the whole training loop is contained in the .fit() method. In PyTorch, however, you implement the loop manually. While this provides control over loop's content, it requires a custom implementation.\n",
    "\n",
    "You will write a training loop every time you train a deep learning model with PyTorch, which you'll practice in this exercise. The `show_results()` function provided will display some sample ground truth and the model predictions.\n",
    "\n",
    "The package imports provided are: pandas as `pd`, `torch`, `torch.nn` as `nn`, `torch.optim` as `optim`, as well as `DataLoader` and `TensorDataset` from `torch.utils.data`.\n",
    "\n",
    "The following variables have been created: `dataloader`, containing the dataloader; `model`, containing the neural network; `criterion`, containing the loss function, `nn.MSELoss()`; `optimizer`, containing the SGD optimizer; and `num_epochs`, containing the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the number of epochs and then the dataloader\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Run a forward pass\n",
    "        feature, target = data\n",
    "        prediction = model(feature)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(prediction, target)\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Neural Network Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing ReLU\n",
    "\n",
    "The rectified linear unit (or ReLU) function is one of the most common activation functions in deep learning.\n",
    "\n",
    "It overcomes the training problems linked with the sigmoid function you learned, such as the *vanishing gradients problem*.\n",
    "\n",
    "In this exercise, you'll begin with a ReLU implementation in PyTorch. Next, you'll calculate the gradients of the function.\n",
    "\n",
    "The `nn` module has already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing leaky ReLU\n",
    "\n",
    "You've learned that ReLU is one of the most used activation functions in deep learning. You will find it in modern architecture. However, it does have the inconvenience of outputting null values for negative inputs and therefore, having null gradients. Once an element of the input is negative, it will be set to zero for the rest of the training. Leaky ReLU overcomes this challenge by using a multiplying factor for negative inputs.\n",
    "\n",
    "In this exercise, you will implement the leaky ReLU function in NumPy and PyTorch and practice using it. The `numpy` as `np` package, the `torch` package as well as the `torch.nn` as `nn` have already been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1000)\n"
     ]
    }
   ],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding activation functions\n",
    "\n",
    "You've learned all about ReLU vs. leaky ReLU. Which of the following statements are true?\n",
    "\n",
    "- [X] The ReLU activation function only outputs zero for negative inputs.\n",
    "- [ ] Sigmoid activation functions are often found after each linear layer in a network.\n",
    "- [X] ReLU(x) = 3 for x = 3.\n",
    "- [ ] Leaky ReLU outputs the same constant value for negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of parameters\n",
    "\n",
    "Deep learning models are famous for having a lot of parameters. Recent language models have billions of parameters. With more parameters comes more computational complexity and longer training times, and a deep learning practitioner must know how many parameters their model has.\n",
    "\n",
    "In this exercise, you will calculate the number of parameters in your model, first using PyTorch then manually.\n",
    "\n",
    "The `torch.nn` package has been imported as `nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 4),\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for parameter in model.parameters():\n",
    "    total += parameter.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Calculate manually the number of parameters of the model below. How many does it have?\n",
    "```py\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 4),\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "Answer: 4\\*(16+1) + 2\\*(4+1) + 1\\*(2+1) = **81**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating the capacity of a network\n",
    "\n",
    "In this exercise, you will practice creating neural networks with different capacities. The capacity of a network reflects the number of parameters in said network. To help you, a `calculate_capacity()` function has been implemented.\n",
    "\n",
    "This function returns the number of parameters in the your model.\n",
    "\n",
    "The dataset you are training this network on has `n_features` features and `n_classes` classes. The `torch.nn` package has been imported as `nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_capacity(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "    return total\n",
    "\n",
    "n_features = 8\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 5),\n",
    "    nn.Linear(5, 7),\n",
    "    nn.Linear(7, n_classes)\n",
    ")\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network with more than 120 parameters\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 7),\n",
    "    nn.Linear(7, 7),\n",
    "    nn.Linear(7, 7),\n",
    "    nn.Linear(7, n_classes)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with learning rate\n",
    "\n",
    "In this exercise, your goal is to find the optimal learning rate such that the optimizer can find the minimum of the non-convex function `x^4 + x^3 - 5x^2` in ten steps.\n",
    "\n",
    "You will experiment with three different learning rate values. For this problem, try learning rate values between 0.001 to 0.1.\n",
    "\n",
    "You are provided with the `optimize_and_plot()` function that takes the learning rate for the first argument. This function will run 10 steps of the SGD optimizer and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a first learning rate value\n",
    "lr0 = 0.007\n",
    "optimize_and_plot(lr=lr0)\n",
    "\n",
    "# Try a second learning rate value\n",
    "lr1 = 0.7\n",
    "optimize_and_plot(lr=lr1)\n",
    "\n",
    "# Try a third learning rate value\n",
    "lr2 = 0.09\n",
    "optimize_and_plot(lr=lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with momentum\n",
    "\n",
    "In this exercise, your goal is to find the optimal momentum such that the optimizer can find the minimum of the following non-convex function `xË†4 + x^3 - 5x^2` in 20 steps. You will experiment with two different momentum values. For this problem, the learning rate is fixed at 0.01.\n",
    "\n",
    "You are provided with the `optimize_and_plot()` function that takes the momentum for the first argument. This function will run 20 steps of the SGD optimizer and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a first value for momentum\n",
    "mom0 = 0.7\n",
    "optimize_and_plot(momentum=mom0)\n",
    "\n",
    "# Try a second value for momentum\n",
    "mom1 = 0.97\n",
    "optimize_and_plot(momentum=mom1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning process\n",
    "\n",
    "You are training a model on a new dataset and you think you can use a fine-tuning approach instead of training from scratch (i.e., training from randomly initialized weights).\n",
    "\n",
    "To fine-tune a model on a new task, you need to follow a certain number of steps that are listed here.\n",
    "\n",
    "1. Find a model trained on a similar task\n",
    "2. Load pre-trained weights\n",
    "3. Freeze (or not) some of the layers in the model\n",
    "4. Train with a smaller learning rate\n",
    "5. Look at the loss values and see if the learning rate needs to be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze layers of a model\n",
    "\n",
    "You are about to fine-tune a model on a new task after loading pre-trained weights. The model contains three linear layers. However, because your dataset is small, you only want to train the last linear layer of this model and freeze the first two linear layers.\n",
    "\n",
    "The model has already been created and exists under the variable `model`. You will be using the `named_parameters` method of the model to list the parameters of the model. Each parameter is described by a name. This name is a string with the following naming convention: `x.name` where `x` is the index of the layer.\n",
    "\n",
    "Remember that a linear layer has two parameters: the `weight` and the `bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():    \n",
    "  \n",
    "    # Check if the parameters belong to the first layer\n",
    "    if name == '0.weight' or name == '0.bias':\n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n",
    "  \n",
    "    # Check if the parameters belong to the second layer\n",
    "    if name == '1.weight' or name == '1.bias':\n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer initialization\n",
    "\n",
    "The initialization of the weights of a neural network has been the focus of researchers for many years. When training a network, the method used to initialize the weights has a direct impact on the final performance of the network.\n",
    "\n",
    "As a machine learning practitioner, you should be able to experiment with different initialization strategies. In this exercise, you are creating a small neural network made of two layers and you are deciding to initialize each layer's weights with the uniform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Use uniform initialization for layer0 and layer1 weights\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(layer0, layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Evaluating and Improving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the TensorDataset class\n",
    "\n",
    "In practice, loading your data into a PyTorch dataset will be one of the first steps you take in order to create and train a neural network with PyTorch.\n",
    "\n",
    "The `TensorDataset` class is very helpful when your dataset can be loaded directly as a NumPy array. Recall that `TensorDataset()` can take one or more NumPy arrays as input.\n",
    "\n",
    "In this exercise, you'll practice creating a PyTorch dataset using the TensorDataset class.\n",
    "\n",
    "`torch` and `numpy` have already been imported for you, along with the `TensorDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1732, 0.7205, 0.8219, 0.0641, 0.9521, 0.1099, 0.8399, 0.5873],\n",
      "       dtype=torch.float64), tensor([0.6428], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features)\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features, torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From data loading to running a forward pass\n",
    "\n",
    "In this exercise, you'll create a PyTorch `DataLoader` from a pandas DataFrame and call a model on this dataset. Specifically, you'll run a *forward pass* on a neural network. You'll continue working with fully connected neural networks, as you have done so far.\n",
    "\n",
    "You'll begin by subsetting a loaded DataFrame called `dataframe`, converting features and targets NumPy arrays, and converting to PyTorch tensors in order to create a PyTorch dataset.\n",
    "\n",
    "This dataset can be loaded into a PyTorch `DataLoader`, batched, shuffled, and used to run a forward pass on a custom fully connected neural network.\n",
    "\n",
    "NumPy as `np`, pandas as `pd`, `torch`, `TensorDataset()`, and `DataLoader()` have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(np.array(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']])).float()\n",
    "target = torch.tensor(np.array(dataframe.Potability)).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)\n",
    "\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "# Create a model using the nn.Sequential API\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 4),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "output = model(features)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the evaluation loop\n",
    "\n",
    "In this exercise, you will practice writing the evaluation loop. Recall that the evaluation loop is similar to the training loop, except that you will not perform the gradient calculation and the optimizer step.\n",
    "\n",
    "The `model` has already been defined for you, along with the object `validationloader`, which is a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in validationloader:\n",
    "        outputs = model(data[0])\n",
    "        loss = criterion(outputs, data[1])\n",
    "        \n",
    "        # Sum the current loss to the validation_loss variable\n",
    "        validation_loss += loss.item()\n",
    "      \n",
    "# Calculate the mean loss value\n",
    "validation_loss_epoch = validation_loss/len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating accuracy using torchmetrics\n",
    "\n",
    "In addition to the losses, you should also be keeping track of the accuracy during training. By doing so, you will be able to select the epoch when the model performed the best.\n",
    "\n",
    "In this exercise, you will practice using the `torchmetrics` package to calculate the accuracy. You will be using a sample of the facemask dataset. This dataset contains three different classes. The `plot_errors` function will display samples where the model predictions do not match the ground truth. Performing such error analysis will help you understand your model failure modes.\n",
    "\n",
    "The `torchmetrics` package is already imported. The model `outputs` are the probabilities returned by a softmax as the last step of the model. The `labels` tensor contains the labels as one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "# Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task='multiclass', num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "\n",
    "# Reset the metric for the next epoch \n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with dropout\n",
    "\n",
    "The dropout layer randomly zeroes out elements of the input tensor. Doing so helps fight overfitting. In this exercise, you'll create a small neural network with at least two linear layers, two dropout layers, and two activation functions.\n",
    "\n",
    "The `torch.nn` package has already been imported as `nn`. An `input_tensor` of dimensions 1 x 3072 has been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small neural network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout()\n",
    ")\n",
    "model(input_tensor)\n",
    "\n",
    "# Using the same model, set the dropout probability to 0.8\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.8)\n",
    ")\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding overfitting\n",
    "\n",
    "Overfitting is very common in machine learning, where the trend is for bigger and bigger models. As a machine learning practitioner, you will face overfitting in your career. Which of the following statements about overfitting are true?\n",
    "\n",
    "- [X] Overfitting happens when the model is performing worse on the validation set than on the training set.\n",
    "- [X] Data augmentation can reduce overfitting by artificially increasing the size of the training set.\n",
    "- [ ] Using a weight decay factor of zero will reduce overfitting.\n",
    "- [X] A dropout layer with a probability strictly superior to zero will reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing random search\n",
    "\n",
    "Hyperparameter search is a computationally costly approach to experiment with different hyperparameter values. However, it can lead to performance improvements. In this exercise, you will implement a random search algorithm.\n",
    "\n",
    "You will randomly sample 10 values of the learning rate and momentum from the uniform distribution. To do so, you will use the `np.random.uniform()` function.\n",
    "\n",
    "The `numpy` package has already been imported as `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for idx in range(10):\n",
    "    # Randomly sample a learning rate factor between 2 and 4\n",
    "    factor = np.random.uniform(2, 4)\n",
    "    lr = 10 ** -factor\n",
    "    \n",
    "    # Randomly select a momentum between 0.85 and 0.99\n",
    "    momentum = np.random.uniform(0.85, 0.99)\n",
    "    \n",
    "    values.append((lr, momentum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
